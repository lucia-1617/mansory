{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911404f0-d2ae-4c5a-9824-194252adf602",
   "metadata": {},
   "source": [
    "**Limitar uso de CPU (ejecútala ANTES de importar TF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae6ff01-74e0-4a75-8f14-0ae7c363b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 0 — limita hilos (menos 100% saturado)\n",
    "import os, multiprocessing\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"           # BLAS/NumPy\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"2\"    # TF kernels internos\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"2\"    # entre ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b704ae-cd54-4516-b5d6-1d7fb28f1e26",
   "metadata": {},
   "source": [
    "**Imports y versión**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ade529f-20df-47ed-adb9-f681de93574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Celda 1\n",
    "import json, math, random, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c351c6-0912-4074-82eb-d7270a42ab15",
   "metadata": {},
   "source": [
    "**Rutas y parámetros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ef9d84-647f-4cd2-a143-bf46465c9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2\n",
    "PROJECT_ROOT = Path.cwd()          # C:\\Users\\User\\mansory\n",
    "TRAIN_DIR    = PROJECT_ROOT / \"train\"\n",
    "VAL_DIR      = PROJECT_ROOT / \"test\"\n",
    "\n",
    "MODEL_DIR     = PROJECT_ROOT / \"modelos\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "MODEL_REGULAR = MODEL_DIR / \"modelo_mansory.keras\"\n",
    "MODEL_BEST95  = MODEL_DIR / \"modelo_mansory_95.keras\"\n",
    "CLASSES_JSON  = MODEL_DIR / \"classes.json\"\n",
    "\n",
    "# Imagen / batch\n",
    "IMG_SIZE = (160, 160)\n",
    "BATCH    = 64                      # baja a 32 si te falta RAM\n",
    "\n",
    "# Tamaño efectivo por época (para que no dure horas en CPU)\n",
    "EFFECTIVE_IMAGES_PER_EPOCH = 12000\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc719cf6-9652-4c4f-9194-3411b2693ef4",
   "metadata": {},
   "source": [
    "**Escaneo de archivos con downsampling de “sin_grietas”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820c8db6-37dc-4d23-b504-7048e5abba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases (orden):\n",
      " 0 -> compresion_vertical_Grave\n",
      " 1 -> compresion_vertical_Leve\n",
      " 2 -> compresion_vertical_Moderada\n",
      " 3 -> friccion_cortante_escalonada_Grave\n",
      " 4 -> friccion_cortante_escalonada_Leve\n",
      " 5 -> friccion_cortante_escalonada_Moderada\n",
      " 6 -> sin_grietas\n",
      " 7 -> tension_diagonal_inclinadas_Grave\n",
      " 8 -> tension_diagonal_inclinadas_Leve\n",
      " 9 -> tension_diagonal_inclinadas_Moderada\n",
      "\n",
      "Train por clase: [558, 44, 1376, 660, 56, 3918, 3000, 36, 6, 462]\n",
      "Val   por clase: [38, 44, 214, 74, 64, 426, 23348, 22, 6, 36]\n"
     ]
    }
   ],
   "source": [
    "# Celda 3\n",
    "# Celda 3 — ESCANEO con mapeo exacto de carpetas\n",
    "\n",
    "# Nombres canónicos de clases (10) que usaremos en el modelo\n",
    "CLASS_NAMES = [\n",
    "    \"compresion_vertical_Grave\",\n",
    "    \"compresion_vertical_Leve\",\n",
    "    \"compresion_vertical_Moderada\",\n",
    "    \"friccion_cortante_escalonada_Grave\",\n",
    "    \"friccion_cortante_escalonada_Leve\",\n",
    "    \"friccion_cortante_escalonada_Moderada\",\n",
    "    \"sin_grietas\",\n",
    "    \"tension_diagonal_inclinadas_Grave\",\n",
    "    \"tension_diagonal_inclinadas_Leve\",\n",
    "    \"tension_diagonal_inclinadas_Moderada\",\n",
    "]\n",
    "\n",
    "# Mapeo de prefijo de clase → nombre EXACTO de la carpeta en disco\n",
    "FOLDER_MAP = {\n",
    "    \"compresion_vertical\":           \"compresion_vertical\",\n",
    "    \"friccion_cortante_escalonada\":  \"friccion cortante_escalonada\",  # ojo: espacio + _\n",
    "    \"sin_grietas\":                   \"sin grietas\",                    # espacio\n",
    "    \"tension_diagonal_inclinadas\":   \"tension diagonal_inclinadas\",   # espacio + _\n",
    "}\n",
    "\n",
    "# Límite por clase en TRAIN (downsample fuerte de 'sin_grietas' para equilibrar)\n",
    "CAPS_TRAIN = {\"sin_grietas\": 3000}\n",
    "CAPS_VAL   = {}   # sin límite\n",
    "\n",
    "EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".JPG\",\".JPEG\",\".PNG\",\".BMP\")\n",
    "\n",
    "def split_prefix_sev(class_name: str):\n",
    "    \"\"\"'compresion_vertical_Grave' -> ('compresion_vertical','Grave')\"\"\"\n",
    "    if class_name == \"sin_grietas\":\n",
    "        return \"sin_grietas\", None\n",
    "    pref, sev = class_name.rsplit(\"_\", 1)\n",
    "    return pref, sev\n",
    "\n",
    "def scan_with_map(root: Path, caps: dict):\n",
    "    files, labels = [], []\n",
    "    counts = []\n",
    "\n",
    "    for idx, cname in enumerate(CLASS_NAMES):\n",
    "        pref, sev = split_prefix_sev(cname)\n",
    "        folder = FOLDER_MAP[pref]  # nombre real en disco\n",
    "\n",
    "        if cname == \"sin_grietas\":\n",
    "            d = root / folder\n",
    "            imgs = []\n",
    "            for e in EXTS: imgs += list(d.glob(f\"*{e}\"))\n",
    "        else:\n",
    "            d = root / folder / sev\n",
    "            imgs = []\n",
    "            for e in EXTS: imgs += list(d.glob(f\"*{e}\"))\n",
    "\n",
    "        # control de existencia para depurar\n",
    "        if not d.exists():\n",
    "            print(f\"⚠️  Carpeta NO encontrada: {d}\")\n",
    "\n",
    "        # límite por clase si aplica\n",
    "        cap = caps.get(cname, None) if caps else None\n",
    "        if cap and len(imgs) > cap:\n",
    "            imgs = imgs[:cap]\n",
    "\n",
    "        files.extend(imgs)\n",
    "        labels.extend([idx] * len(imgs))\n",
    "        counts.append(len(imgs))\n",
    "\n",
    "    return files, labels, np.array(counts, dtype=np.int64)\n",
    "\n",
    "train_files, train_labels, train_counts = scan_with_map(TRAIN_DIR, CAPS_TRAIN)\n",
    "val_files,   val_labels,   val_counts   = scan_with_map(VAL_DIR,   CAPS_VAL)\n",
    "\n",
    "print(\"Clases (orden):\")\n",
    "for i, n in enumerate(CLASS_NAMES):\n",
    "    print(f\"{i:2d} -> {n}\")\n",
    "\n",
    "print(\"\\nTrain por clase:\", train_counts.tolist())\n",
    "print(\"Val   por clase:\", val_counts.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce44496-1a48-46d1-9155-4b1fbc1d3a10",
   "metadata": {},
   "source": [
    "**Dataset rápido (cache, prefetch) y preprocesado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d0f286-eee7-48ab-be6f-a1bb5b32ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4\n",
    "def load_and_preprocess(path, label, training: bool):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)      # [0,1]\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    if training:\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, 0.08)\n",
    "        img = tf.image.random_contrast(img, 0.9, 1.1)\n",
    "    # Escala [-1,1] (MobileNetV2)\n",
    "    img = (img - 0.5) * 2.0\n",
    "    return img, label\n",
    "\n",
    "def make_ds(files, labels, training=True):\n",
    "    files = list(map(str, files))\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(files), 10000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda p,l: load_and_preprocess(p,l,training),\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(train_files, train_labels, training=True)\n",
    "val_ds   = make_ds(val_files,   val_labels,   training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7f3af-7476-4ca9-87b0-e0dec529af80",
   "metadata": {},
   "source": [
    "**Modelo (MobileNetV2 liviano) + Focal Loss con α**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d99d4c-a616-4ece-b579-83942451c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: [0.25, 0.9399999976158142, 0.25, 0.25, 0.7379999756813049, 0.25, 0.25, 1.1490000486373901, 4.0, 0.25]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mansory_mobilenetv2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mansory_mobilenetv2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_0.35_160            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">410,208</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_0.35_160            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │       \u001b[38;5;34m410,208\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">423,018</span> (1.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m423,018\u001b[0m (1.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">410,208</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m410,208\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 5\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "# alpha (inversa de frecuencia, normalizada y acotada)\n",
    "counts = train_counts + 1e-9\n",
    "alpha  = (counts.sum() / (num_classes * counts))\n",
    "alpha  = alpha / alpha.mean()\n",
    "alpha  = np.clip(alpha, 0.25, 4.0).astype(\"float32\")\n",
    "print(\"alpha:\", np.round(alpha, 3).tolist())\n",
    "\n",
    "def focal_loss(alpha_vec, gamma=2.0):\n",
    "    alpha_const = tf.constant(alpha_vec, dtype=tf.float32)\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0-1e-7)\n",
    "        ce  = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1)\n",
    "        pt  = tf.reduce_sum(y_true * y_pred, axis=1)\n",
    "        at  = tf.reduce_sum(y_true * alpha_const, axis=1)\n",
    "        fl  = at * tf.pow(1.0 - pt, gamma) * ce\n",
    "        return tf.reduce_mean(fl)\n",
    "    return loss\n",
    "\n",
    "base = keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SIZE+(3,), include_top=False, weights=\"imagenet\", alpha=0.35\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs  = keras.Input(shape=IMG_SIZE+(3,), name=\"image\")\n",
    "x       = base(inputs, training=False)\n",
    "x       = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x       = keras.layers.Dropout(0.25)(x)\n",
    "outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model   = keras.Model(inputs, outputs, name=\"mansory_mobilenetv2\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=focal_loss(alpha, gamma=2.0),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91176c53-d062-4322-9498-cf70deb3dc7c",
   "metadata": {},
   "source": [
    "**Callbacks y fit (épocas cortas en CPU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1a14cd-907d-4d00-b59a-9b63be1766a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS_PER_EPOCH: 187\n",
      "Epoch 1/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 3s/step - accuracy: 0.7935 - loss: 0.0852 - val_accuracy: 0.9277 - val_loss: 0.0405 - learning_rate: 2.5000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 3s/step - accuracy: 0.8090 - loss: 0.0751 - val_accuracy: 0.9219 - val_loss: 0.0406 - learning_rate: 2.5000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 3s/step - accuracy: 0.8178 - loss: 0.0687 - val_accuracy: 0.9205 - val_loss: 0.0426 - learning_rate: 2.5000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 3s/step - accuracy: 0.8285 - loss: 0.0632 - val_accuracy: 0.9348 - val_loss: 0.0352 - learning_rate: 1.2500e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 3s/step - accuracy: 0.8307 - loss: 0.0604 - val_accuracy: 0.9282 - val_loss: 0.0381 - learning_rate: 1.2500e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 3s/step - accuracy: 0.8385 - loss: 0.0559 - val_accuracy: 0.9227 - val_loss: 0.0395 - learning_rate: 1.2500e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 3s/step - accuracy: 0.8407 - loss: 0.0570 - val_accuracy: 0.9272 - val_loss: 0.0381 - learning_rate: 6.2500e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 3s/step - accuracy: 0.8353 - loss: 0.0555 - val_accuracy: 0.9148 - val_loss: 0.0426 - learning_rate: 6.2500e-05\n",
      "Epoch 9/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 3s/step - accuracy: 0.8446 - loss: 0.0544 - val_accuracy: 0.9221 - val_loss: 0.0404 - learning_rate: 3.1250e-05\n",
      "\n",
      "Clases guardadas en C:\\Users\\User\\mansory\\modelos\\classes.json\n",
      "Mejor val_accuracy: 0.935\n",
      "Modelo mejor por val_acc: C:\\Users\\User\\mansory\\modelos\\modelo_mansory.keras\n",
      "También se guardó ≥95%: C:\\Users\\User\\mansory\\modelos\\modelo_mansory_95.keras\n"
     ]
    }
   ],
   "source": [
    "# Celda 6\n",
    "class SaveBest95(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = (logs or {}).get(\"val_accuracy\")\n",
    "        if val_acc is not None and val_acc >= 0.95:\n",
    "            self.model.save(MODEL_BEST95)\n",
    "            print(f\"\\n✅ Guardado >=95%: {MODEL_BEST95}\")\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_accuracy\"),\n",
    "    keras.callbacks.ModelCheckpoint(MODEL_REGULAR, save_best_only=True, monitor=\"val_accuracy\"),\n",
    "    SaveBest95(),\n",
    "]\n",
    "\n",
    "STEPS_PER_EPOCH = max(1, EFFECTIVE_IMAGES_PER_EPOCH // BATCH)\n",
    "print(\"STEPS_PER_EPOCH:\", STEPS_PER_EPOCH)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.repeat(),                 # permite steps_per_epoch\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# guardar nombres de clases\n",
    "with open(CLASSES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"class_names\": CLASS_NAMES}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nClases guardadas en\", CLASSES_JSON)\n",
    "best_val = max(history.history.get(\"val_accuracy\", [0.0]))\n",
    "print(f\"Mejor val_accuracy: {best_val:.3f}\")\n",
    "print(\"Modelo mejor por val_acc:\", MODEL_REGULAR)\n",
    "if MODEL_BEST95.exists():\n",
    "    print(\"También se guardó ≥95%:\", MODEL_BEST95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112affdc-3b2a-441b-91ad-952d09595f78",
   "metadata": {},
   "source": [
    "**Inferencia (tipo + severidad SIN porcentajes)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b9528f4-4b04-40c3-bcc3-e72acc6ccde5",
   "metadata": {},
   "source": [
    "# Celda 7\n",
    "def split_type_severity(name: str):\n",
    "    if name == \"sin_grietas\":\n",
    "        return \"sin grietas\", \"N/A\"\n",
    "    if name.startswith(\"compresion_vertical_\"):\n",
    "        return \"compresión vertical\", name.split(\"_\")[-1]\n",
    "    if name.startswith(\"friccion cortante_escalonada_\"):\n",
    "        return \"fricción cortante escalonada\", name.split(\"_\")[-1]\n",
    "    if name.startswith(\"tension diagonal_inclinadas_\"):\n",
    "        return \"tensión diagonal inclinadas\", name.split(\"_\")[-1]\n",
    "    return name, \"\"\n",
    "\n",
    "def predict_image(img_path: Path):\n",
    "    # usa modelo ≥95% si existe; si no, el mejor por val_acc\n",
    "    model_path = MODEL_BEST95 if MODEL_BEST95.exists() else MODEL_REGULAR\n",
    "    print(\"Usando:\", model_path)\n",
    "    m = keras.models.load_model(str(model_path), compile=False)\n",
    "\n",
    "    img = keras.utils.load_img(str(img_path), target_size=IMG_SIZE)\n",
    "    x   = keras.utils.img_to_array(img) / 255.0\n",
    "    x   = (x - 0.5) * 2.0\n",
    "    x   = np.expand_dims(x, 0)\n",
    "\n",
    "    p = m.predict(x, verbose=0)[0]\n",
    "    pred_id = int(np.argmax(p))\n",
    "    name = CLASS_NAMES[pred_id]\n",
    "    tipo, sev = split_type_severity(name)\n",
    "    print(f\"\\nArchivo: {img_path}\")\n",
    "    print(f\"Tipo de grieta: {tipo}\")\n",
    "    print(f\"Severidad: {sev}\")\n",
    "\n",
    "# ejemplo rápido (cámbialo por la que quieras):\n",
    "predict_image(PROJECT_ROOT / \"55.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mansorytf]",
   "language": "python",
   "name": "conda-env-mansorytf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
